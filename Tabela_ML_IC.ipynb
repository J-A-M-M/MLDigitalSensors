{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KkoIWjF5sYJ7",
        "g4FA1Rc4smxR",
        "bDXgL2_4QsER",
        "0HprU1CcDpv7",
        "LvztMlMFDuqg",
        "pM8GLJmIjUiR"
      ],
      "authorship_tag": "ABX9TyMxC4VxRyriGtEeac2rO17b"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tabela Machine Learning - IC"
      ],
      "metadata": {
        "id": "KkoIWjF5sYJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import mlab\n",
        "from scipy import signal\n",
        "from scipy.fft import fftshift\n",
        "from tqdm import tqdm\n",
        "from scipy.signal import butter, lfilter\n",
        "import seaborn as sns\n",
        "import os\n",
        "import math as m\n",
        "import shutil\n",
        "import random\n",
        "import statistics as stc\n",
        "from statistics import variance\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import kurtosis"
      ],
      "metadata": {
        "id": "iGmzY5nusbQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Hntub4W1sxch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Data"
      ],
      "metadata": {
        "id": "g4FA1Rc4smxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_grav = '/content/drive/My Drive/IC/ColabNotebooks/Vibracoes/DataFrames/Gravidade/'\n",
        "filename = os.listdir( path_grav ) #Reading all file names inside the folder/path\n",
        "filename #List of file names"
      ],
      "metadata": {
        "id": "KeiMVIeascAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_normal = '/content/drive/My Drive/IC/ColabNotebooks/Vibracoes/DataFrames/22-04-2023-Normal/'\n",
        "filenames = os.listdir( path_normal ) #Reading all file names inside the folder/path\n",
        "filenames #List of file names"
      ],
      "metadata": {
        "id": "19Nsk2pls3oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_desbal1_1 = '/content/drive/My Drive/IC/ColabNotebooks/Vibracoes/DataFrames/10-05-2023-Desbalanceamento1/1_massa/'\n",
        "filenames1_1 = os.listdir( path_desbal1_1 ) #Reading all file names inside the folder/path\n",
        "filenames1_1 #List of file names"
      ],
      "metadata": {
        "id": "fIPRa1D-s3lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_desbal1_2 = '/content/drive/My Drive/IC/ColabNotebooks/Vibracoes/DataFrames/10-05-2023-Desbalanceamento1/2_massa/'\n",
        "filenames1_2 = os.listdir( path_desbal1_2 ) #Reading all file names inside the folder/path\n",
        "filenames1_2 #List of file names"
      ],
      "metadata": {
        "id": "JGihY8LOs3i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_desbal1_3 = '/content/drive/My Drive/IC/ColabNotebooks/Vibracoes/DataFrames/10-05-2023-Desbalanceamento1/3_massa/'\n",
        "filenames1_3 = os.listdir( path_desbal1_3 ) #Reading all file names inside the folder/path\n",
        "filenames1_3 #List of file names"
      ],
      "metadata": {
        "id": "f0qKAgbps3ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_desbal2_1 = '/content/drive/My Drive/IC/ColabNotebooks/Vibracoes/DataFrames/17-05-2023-Desbalanceamento2/1_massa/'\n",
        "filenames2_1 = os.listdir( path_desbal2_1 ) #Reading all file names inside the folder/path\n",
        "filenames2_1 #List of file names"
      ],
      "metadata": {
        "id": "hXjq7FLfs3dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_desbal2_2 = '/content/drive/My Drive/IC/ColabNotebooks/Vibracoes/DataFrames/17-05-2023-Desbalanceamento2/2_massa/'\n",
        "filenames2_2 = os.listdir( path_desbal2_2 ) #Reading all file names inside the folder/path\n",
        "filenames2_2 #List of file names"
      ],
      "metadata": {
        "id": "t42mvSrLs3am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_desbal2_3 = '/content/drive/My Drive/IC/ColabNotebooks/Vibracoes/DataFrames/17-05-2023-Desbalanceamento2/3_massa/'\n",
        "filenames2_3 = os.listdir( path_desbal2_3 ) #Reading all file names inside the folder/path\n",
        "filenames2_3 #List of file names"
      ],
      "metadata": {
        "id": "hKMa2XjNs3VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting gravity:\n",
        "for names in filename:\n",
        "    Gravity=pd.read_csv(path_grav + names, index_col=0,sep='\\t')\n",
        "\n",
        "#Other dataframes:\n",
        "DataframesN={} #Dictionary of all datasets for normal conditions. The keys are the frequency and the informations are the dataframes for its corresponding frequency\n",
        "for names in filenames:\n",
        "    DataframesN[names.rstrip('.csv')]=pd.read_csv(path_normal + names, index_col=0,sep='\\t') #The strip function is only for removing \".csv\" from the key's name\n",
        "\n",
        "DataframesD1_1={} #Dictionary of all datasets for unbalance conditions - type 1, mass 1. The keys are the frequency and the informations are the dataframes for its corresponding frequency\n",
        "for names in filenames1_1:\n",
        "    DataframesD1_1[names.rstrip('.csv')]=pd.read_csv(path_desbal1_1 + names, index_col=0,sep='\\t') #The strip function is only for removing \".csv\" from the key's name\n",
        "\n",
        "DataframesD1_2={} #Dictionary of all datasets for unbalance conditions - type 1, mass 2. The keys are the frequency and the informations are the dataframes for its corresponding frequency\n",
        "for names in filenames1_2:\n",
        "    DataframesD1_2[names.rstrip('.csv')]=pd.read_csv(path_desbal1_2 + names, index_col=0,sep='\\t') #The strip function is only for removing \".csv\" from the key's name\n",
        "\n",
        "DataframesD1_3={} #Dictionary of all datasets for unbalance conditions - type 1, mass 3. The keys are the frequency and the informations are the dataframes for its corresponding frequency\n",
        "for names in filenames1_3:\n",
        "    DataframesD1_3[names.rstrip('.csv')]=pd.read_csv(path_desbal1_3 + names, index_col=0,sep='\\t') #The strip function is only for removing \".csv\" from the key's name\n",
        "\n",
        "DataframesD2_1={} #Dictionary of all datasets for unbalance conditions - type 2, mass 1. The keys are the frequency and the informations are the dataframes for its corresponding frequency\n",
        "for names in filenames2_1:\n",
        "    DataframesD2_1[names.rstrip('.csv')]=pd.read_csv(path_desbal2_1 + names, index_col=0,sep='\\t') #The strip function is only for removing \".csv\" from the key's name\n",
        "\n",
        "DataframesD2_2={} #Dictionary of all datasets for unbalance conditions - type 2, mass 2. The keys are the frequency and the informations are the dataframes for its corresponding frequency\n",
        "for names in filenames2_2:\n",
        "    DataframesD2_2[names.rstrip('.csv')]=pd.read_csv(path_desbal2_2 + names, index_col=0,sep='\\t') #The strip function is only for removing \".csv\" from the key's name\n",
        "\n",
        "DataframesD2_3={} #Dictionary of all datasets for unbalance conditions - type 2, mass 3. The keys are the frequency and the informations are the dataframes for its corresponding frequency\n",
        "for names in filenames2_3:\n",
        "    DataframesD2_3[names.rstrip('.csv')]=pd.read_csv(path_desbal2_3 + names, index_col=0,sep='\\t') #The strip function is only for removing \".csv\" from the key's name\n"
      ],
      "metadata": {
        "id": "kdMoSW1Gs3Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Informations:\n",
        "Dictionaries_names=['DataframesN','DataframesD1_1','DataframesD1_2','DataframesD1_3','DataframesD2_1','DataframesD2_2','DataframesD2_3'] #IMPORTANT INFORMATION (MANUALLY INFORM AFTER ADDING OTHER DICTIONARY OF DATAFRAMES)\n",
        "a_X=['imu1accx','imu2accx','imu3accx'] # information of X-acceleration\n",
        "a_Y=['imu1accy','imu2accy','imu3accy'] # information of Y-acceleration\n",
        "a_Z=['imu1accz','imu2accz','imu3accz'] # information of Z-acceleration\n",
        "g_X=['imu1gyrox','imu2gyrox','imu3gyrox'] # information of X-gyroscope\n",
        "g_Y=['imu1gyroy','imu2gyroy','imu3gyroy'] # information of Y-gyroscope\n",
        "g_Z=['imu1gyroz','imu2gyroz','imu3gyroz'] # information of Z-gyroscope\n",
        "M_1_a=['imu1accx','imu1accy','imu1accz']\n",
        "M_2_a=['imu2accx','imu2accy','imu2accz']\n",
        "M_3_a=['imu3accx','imu3accy','imu3accz']\n",
        "M_1_g=['imu1gyrox','imu1gyroy','imu1gyroz']\n",
        "M_2_g=['imu2gyrox','imu2gyroy','imu2gyroz']\n",
        "M_3_g=['imu3gyrox','imu3gyroy','imu3gyroz']\n",
        "all=['imu1accx','imu1accy','imu1accz','imu2accx','imu2accy','imu2accz','imu3accx','imu3accy','imu3accz','imu1gyrox','imu1gyroy','imu1gyroz','imu2gyrox','imu2gyroy','imu2gyroz','imu3gyrox','imu3gyroy','imu3gyroz']\n",
        "n_c=['a1_X','a1_Y','a1_Z','a2_X','a2_Y','a2_Z','a3_X','a3_Y','a3_Z','g1_X','g1_Y','g1_Z','g2_X','g2_Y','g2_Z','g3_X','g3_Y','g3_Z']"
      ],
      "metadata": {
        "id": "kJqEfAcCtG9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Deleting garbage information\n",
        "list_remov=['dac1','dac2','dac3','dac4','log']\n",
        "\n",
        "def delgarbage (Dict_of_df): #Give the Name of the Dictionary with Dataframes\n",
        "    for keys in Dict_of_df:\n",
        "      for col_to_remove in list_remov:\n",
        "        Dict_of_df[keys].drop(col_to_remove, axis=1, inplace=True)\n",
        "    remove=list_remov\n",
        "    informations=list(Dict_of_df[list(Dict_of_df)[0]].columns)\n",
        "    return remove, informations\n",
        "\n",
        "\n",
        "#Removing Gravity from the X-axis\n",
        "def removegravity (Dict_of_df):\n",
        "    for keys in Dict_of_df:\n",
        "        for info in a_X:\n",
        "            Dict_of_df[keys][info] -= Gravity[info].mean()\n",
        "    return"
      ],
      "metadata": {
        "id": "Uch9gEgYtJIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loop to remove garbage information / columns\n",
        "for n in Dictionaries_names:\n",
        "    l_removed, l_informations = delgarbage(eval(n))\n",
        "print(\" List of Removed Informations: \",l_removed,\"\\n\",\"List of Useful Informations: \", l_informations)\n"
      ],
      "metadata": {
        "id": "_l0ckaDdtQCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in Dictionaries_names:\n",
        "    removegravity(eval(n))"
      ],
      "metadata": {
        "id": "96hHbvIktP2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tachometer"
      ],
      "metadata": {
        "id": "bDXgL2_4QsER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Tachometer (Dict_of_df):\n",
        "  rot_list=[]\n",
        "  for keys in Dict_of_df:\n",
        "\n",
        "    df=Dict_of_df[keys][['time','imu1accx','imu1accy','imu1accz','imu2accx','imu2accy','imu2accz','imu3accx','imu3accy','imu3accz','adc1.3','adc2.3','adc3.3','adc4.3']]\n",
        "    df_array=df.values\n",
        "    tach=df[['adc1.3','adc2.3','adc3.3','adc4.3']].values.reshape((df.shape[0]*4))#Tranformando todas as informações do tacometro em 1 coluna só\n",
        "    #print(tach)\n",
        "    tach_size=tach.shape[0]\n",
        "    i=0\n",
        "    new_rot=0\n",
        "    for i in range(1000,tach_size//2,500):\n",
        "      it=0\n",
        "      rot=new_rot\n",
        "      interm=tach[i-500:i]\n",
        "      interm_size=interm.shape[0]\n",
        "      sig1 = (interm > 0.1).astype(int)\n",
        "      sig2 = np.diff(sig1)\n",
        "      idxpicos = np.where(sig2 > 0.5)[0]\n",
        "      # A diferença temporal entre picos indica o perído de rotação:\n",
        "      periodos = (idxpicos[1:] - idxpicos[0:-1]) * 1e-3  # Amostragem de 1 ms, por isso o 1e-3\n",
        "      # Frequencias em RPM:\n",
        "      new_rot = 1 / (np.mean(periodos)) * 60\n",
        "      if abs(new_rot-rot)<1:\n",
        "        if it<2:\n",
        "          it+=1\n",
        "        else:\n",
        "          final_rot=new_rot\n",
        "          break\n",
        "      elif new_rot<rot:\n",
        "        final_rot=rot\n",
        "        break\n",
        "      i+=1\n",
        "    t_sub=df['time'][(i-it)//4]\n",
        "    sig1 = (tach[i-500:i+9500] > 0.1).astype(int)\n",
        "    sig2 = np.diff(sig1)\n",
        "    idxpicos = np.where(sig2 > 0.5)[0]\n",
        "    periodos = (idxpicos[1:] - idxpicos[0:-1]) * 1e-3\n",
        "    rot = 1 / (np.mean(periodos)) * 60\n",
        "    rot_list.append(round(rot,3))\n",
        "    #print(f'Para {keys} o tempo de subida e rotação foram:')\n",
        "    #print(f'O tempo de subida é {t_sub} s')\n",
        "    #print('Ponto/Linha para Primeiro Corte:', t_sub/0.004)\n",
        "    #print('Ponto/Linha para Segundo Corte:', (t_sub/0.004)+2500) #10s/0.004=2500\n",
        "    #print(f'A velocidade máxima de rotação é {rot} rpm')\n",
        "    #print()\n",
        "  return rot_list"
      ],
      "metadata": {
        "id": "yYwOKQewQ0Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Tachometer_for_funct (Dict_of_df, keys):\n",
        "  df=Dict_of_df[keys][['time','imu1accx','imu1accy','imu1accz','imu2accx','imu2accy','imu2accz','imu3accx','imu3accy','imu3accz','adc1.3','adc2.3','adc3.3','adc4.3']]\n",
        "  df_array=df.values\n",
        "  tach=df[['adc1.3','adc2.3','adc3.3','adc4.3']].values.reshape((df.shape[0]*4))#Tranformando todas as informações do tacometro em 1 coluna só\n",
        "  #print(tach)\n",
        "  tach_size=tach.shape[0]\n",
        "  i=0\n",
        "  new_rot=0\n",
        "  for i in range(1000,tach_size//2,500):\n",
        "    it=0\n",
        "    rot=new_rot\n",
        "    interm=tach[i-500:i]\n",
        "    interm_size=interm.shape[0]\n",
        "    sig1 = (interm > 0.1).astype(int)\n",
        "    sig2 = np.diff(sig1)\n",
        "    idxpicos = np.where(sig2 > 0.5)[0]\n",
        "    # A diferença temporal entre picos indica o perído de rotação:\n",
        "    periodos = (idxpicos[1:] - idxpicos[0:-1]) * 1e-3  # Amostragem de 1 ms, por isso o 1e-3\n",
        "    # Frequencias em RPM:\n",
        "    new_rot = 1 / (np.mean(periodos)) * 60\n",
        "    if abs(new_rot-rot)<1:\n",
        "      if it<2:\n",
        "        it+=1\n",
        "      else:\n",
        "        final_rot=new_rot\n",
        "        break\n",
        "    elif new_rot<rot:\n",
        "      final_rot=rot\n",
        "      break\n",
        "    i+=1\n",
        "  t_sub=(df['time'][(i-it)//4])   #Pode-se colocar um adcional para garantir a faixa constante\n",
        "  sig1 = (tach[i-500:i+9500] > 0.1).astype(int)\n",
        "  sig2 = np.diff(sig1)\n",
        "  idxpicos = np.where(sig2 > 0.5)[0]\n",
        "  periodos = (idxpicos[1:] - idxpicos[0:-1]) * 1e-3\n",
        "  rot = 1 / (np.mean(periodos)) * 60\n",
        "  stop1=int(t_sub/0.004)\n",
        "  stop2=int((t_sub/0.004)+2500) #10s/0.004=2500\n",
        "  #print(f'Para {keys} o tempo de subida e rotação foram:')\n",
        "  #print(f'O tempo de subida é {t_sub} s')\n",
        "  #print('Ponto/Linha para Primeiro Corte:', stop1)\n",
        "  #print('Ponto/Linha para Segundo Corte:', stop2)\n",
        "  #print(f'A velocidade máxima de rotação é {rot} rpm')\n",
        "  #print()\n",
        "  return stop1 , stop2, rot"
      ],
      "metadata": {
        "id": "TQdk8CUwQz0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tachometer_for_funct(DataframesN,'sample1')"
      ],
      "metadata": {
        "id": "mWacU9_YRepS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tachometer(DataframesN)"
      ],
      "metadata": {
        "id": "o5pvixpPLKT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Machine Learning Dataframe"
      ],
      "metadata": {
        "id": "wU2BCQeKtgYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time Data"
      ],
      "metadata": {
        "id": "0HprU1CcDpv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RMS(Dict_of_df, data=a_X):\n",
        "  RMS_list=[]\n",
        "  for keys in Dict_of_df:\n",
        "    stop1 , stop2, rot = Tachometer_for_funct(Dict_of_df, keys)\n",
        "    r_list=[]\n",
        "    for info in data:\n",
        "      RMS=np.sqrt(np.mean((Dict_of_df[keys][stop1:stop2][info])**2))\n",
        "      r_list.append(RMS) #Each part of the list is in order with INFO\n",
        "    RMS_list.append(r_list)\n",
        "  return RMS_list #Cada vetor dentro do array é uma linha na ttabela final"
      ],
      "metadata": {
        "id": "G2Vz1yBYtjFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RMS(DataframesN,data=all)"
      ],
      "metadata": {
        "id": "xtm8X27bgeY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FCrista(Dict_of_df, data=a_X):\n",
        "  FC_list=[]\n",
        "  for keys in Dict_of_df:\n",
        "    stop1 , stop2, rot = Tachometer_for_funct(Dict_of_df, keys)\n",
        "    f_list=[]\n",
        "    for info in data:\n",
        "      x_max=(np.abs(Dict_of_df[keys][stop1:stop2][info])).max()\n",
        "      RMS=np.sqrt(np.mean((Dict_of_df[keys][stop1:stop2][info])**2))\n",
        "      FC=x_max/RMS\n",
        "      f_list.append(FC)\n",
        "    FC_list.append(f_list)\n",
        "  return FC_list\n",
        "\n"
      ],
      "metadata": {
        "id": "6jsn9Udmobdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FCrista(DataframesN,data=all)"
      ],
      "metadata": {
        "id": "gNCcgOJKpU5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FImpulso(Dict_of_df, data=a_X):\n",
        "  FI_list=[]\n",
        "  for keys in Dict_of_df:\n",
        "    stop1 , stop2, rot = Tachometer_for_funct(Dict_of_df, keys)\n",
        "    i_list=[]\n",
        "    for info in data:\n",
        "      x_max=(np.abs(Dict_of_df[keys][stop1:stop2][info])).max()\n",
        "      d=np.mean(np.abs((Dict_of_df[keys][stop1:stop2][info])))\n",
        "      FI=x_max/d\n",
        "      i_list.append(FI)\n",
        "    FI_list.append(i_list)\n",
        "  return FI_list"
      ],
      "metadata": {
        "id": "SKwG7zeO9jKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FImpulso(DataframesN,data=all)"
      ],
      "metadata": {
        "id": "x2vPd23_94hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Curtose(Dict_of_df, data=a_X):\n",
        "  KU_list=[]\n",
        "  for keys in Dict_of_df:\n",
        "    stop1 , stop2, rot = Tachometer_for_funct(Dict_of_df, keys)\n",
        "    k_list=[]\n",
        "    for info in data:\n",
        "      KU=kurtosis(Dict_of_df[keys][stop1:stop2][info],fisher=False)\n",
        "      k_list.append(KU)\n",
        "    KU_list.append(k_list)\n",
        "  return KU_list"
      ],
      "metadata": {
        "id": "yA1OzAoT96g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Curtose(DataframesN,data=all)"
      ],
      "metadata": {
        "id": "CepwfqgI_5Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequency Data:"
      ],
      "metadata": {
        "id": "LvztMlMFDuqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing Frequencies Spectrum (Only Magnitude) with Hamming Window:\n",
        "#THE FUNCTION fspectrum ONLY ACCEPT A data list OF 3 VALUES OR MULTIPLES OF 3\n",
        "def fspectrum (Dict_of_df, data=a_X): #'data' is an optional value. If you don't set anything as 'data', the function will use the default list 'informations' obtained in \"delgarbage()\" function\n",
        "    TF_list=[] #Lista de Listas -> TF_list[sample][IMU][Posição/Ponto]\n",
        "    for keys in Dict_of_df: #Accessing the dictionary with the first keys (sample1,...)\n",
        "\n",
        "        #Each key have its maximum time and number of points collected, therefore, the initial calculations for FFT are made:\n",
        "        # Now we are going to get the period of each dataframe / rotational frequency and create a list of spectral frequencies to analyze the frequency domain\n",
        "        # It's notable that we are working with uniform sampling, therefore our sampling frequency will be constant\n",
        "        stop1 , stop2, rot = Tachometer_for_funct(Dict_of_df, keys)\n",
        "\n",
        "        #print(keys)\n",
        "        #print(stop1)\n",
        "        #print(stop2)\n",
        "\n",
        "        #Cutting Lines: (0 to stop1) and (stop2 to last one) /// Only use Dictionary[keys][stop1,stop2][\"WHAT DO YOU WANT\"]\n",
        "\n",
        "        #Collected Data: (Data Information)\n",
        "        t1= Dict_of_df[keys][stop1:stop2]['time'].min()\n",
        "        t2=Dict_of_df[keys][stop1:stop2+1]['time'].max() #Somamos +1 para contar o fim do tempo para próxima amostra\n",
        "        T=t2-t1\n",
        "        #print('T =',T)\n",
        "        N=len(Dict_of_df[keys][stop1:stop2]['time'])\n",
        "        #print(\"N = \",N)\n",
        "        delta_t=T/N\n",
        "        #print(\"delta_t = \",delta_t)\n",
        "        fs=1/delta_t #Sampling frequency (Hz)\n",
        "        #print('fs =',fs)\n",
        "        fh=fs/2 #To evitate “ALIASING”, we set a maximum analyzable frequency, called Nyquist frequency => fmax=fh (Hz)\n",
        "        #print(\"Nyquist Frequency = \", fh)\n",
        "\n",
        "        #Building the Frequency Array (Two forms, but the np.fft.fftfreq is better -> dont gives 0,300000005 -> computational error)\n",
        "        #freq= [(1/T)*j for j in range(N) if ((1/T)*j)<fh] #Using the loop until the Nyquist Frequency (Hz) (use only if np.fft.fftfreq is strange)\n",
        "        freq_compl=np.fft.fftfreq(N,delta_t) #It's better to use this function to calculate the frrquency array, because it gives the correct decimal places. Also, this function gives the positive and negative values of frequencies, if we dive by two we get the Nyquist Frequency\n",
        "        freq=freq_compl[freq_compl>=0] #Here we only cut the positive values of frequencies, notice that this already gives us the frequency array until the Nyquist frequency\n",
        "        #print(\"Max. Frequency = \", max(freq))\n",
        "        #print(\"Length of Frequency Array = \", len(freq))\n",
        "        #print('FREQ:', freq, len(freq))\n",
        "\n",
        "        ffts_abs=[] #Creating a list to append all absulute values FFT list. (list of lists)\n",
        "\n",
        "        #Now accessing each dataframe information:\n",
        "        for info in data:  #Accessing the informations inside each key, using the column name list\n",
        "        #Calculations for the FFT:\n",
        "            fft=((np.fft.fft(Dict_of_df[keys][stop1:stop2][info]*np.hanning(N))))*2/N #Application of FFT to ALL information and dividing by the number of points (With Hanning Window = Correction Factor of 2/N)\n",
        "\n",
        "            #Cutting the FFT, to match with frequencies, remebering that the FFT is symmetrical. Also, getting the absolute values.\n",
        "            #print(\"Length of FFT Array = \", len(fft))\n",
        "            #print(\"Relation between length of Frequencies and FTT arrays, needs to be 2:\", len(fft)/len(freq))\n",
        "            fft_cut=fft[:len(freq)] #Cutting the FTT list until Nyquist Frequency\n",
        "            #print(fft_cut)\n",
        "            #print(\"Relation between length of Frequencies and FTT_CUT arrays, needs to be 1:\", len(fft_cut)/len(freq))\n",
        "            fft_abs=np.abs(fft_cut) #getting the magnitude\n",
        "            ffts_abs.append(fft_abs) #appending to the list\n",
        "        TF_list.append(ffts_abs)\n",
        "    return TF_list\n",
        "\n",
        "#https://www.youtube.com/watch?v=O0Y8FChBaFU\n",
        "#https://www.youtube.com/watch?v=b06pFMIRO0I\n",
        "#https://www.youtube.com/watch?v=1-i4byj3MqI\n",
        "#https://www.monolitonimbus.com.br/transformada-de-fourier-em-python/"
      ],
      "metadata": {
        "id": "sG8tMfQDXTgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fspectrum(DataframesN,data=all)"
      ],
      "metadata": {
        "id": "1M4XMr7wERls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification:"
      ],
      "metadata": {
        "id": "pM8GLJmIjUiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Clas(Dict_of_df):\n",
        "  classi=[]\n",
        "  for keys in Dict_of_df:\n",
        "    if Dict_of_df[keys].equals(DataframesN[keys]):\n",
        "      classi.append('Normal')\n",
        "    elif Dict_of_df[keys].equals(DataframesD1_1[keys]):\n",
        "      classi.append('Desbalanceamento Estático')\n",
        "    elif Dict_of_df[keys].equals(DataframesD1_2[keys]):\n",
        "      classi.append('Desbalanceamento Estático')\n",
        "    elif Dict_of_df[keys].equals(DataframesD1_3[keys]):\n",
        "      classi.append('Desbalanceamento Estático')\n",
        "    elif Dict_of_df[keys].equals(DataframesD2_1[keys]):\n",
        "      classi.append('Desbalanceamento Dinâmico')\n",
        "    elif Dict_of_df[keys].equals(DataframesD2_2[keys]):\n",
        "      classi.append('Desbalanceamento Dinâmico')\n",
        "    elif Dict_of_df[keys].equals(DataframesD2_3[keys]):\n",
        "      classi.append('Desbalanceamento Dinâmico')\n",
        "  return classi"
      ],
      "metadata": {
        "id": "CbTEHNYMjThE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Clas(DataframesD2_2)"
      ],
      "metadata": {
        "id": "asuUItsnUXAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Final Dataset:"
      ],
      "metadata": {
        "id": "G_4NlQcmD1AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Final Dataset:\n",
        "\n",
        "#Final Dictionary:\n",
        "df_ML={}\n",
        "\n",
        "\n",
        "#Lists to create each column, notice that its on order with dictionaries\n",
        "rot=[]\n",
        "clas=[]\n",
        "rms=[]\n",
        "fc=[]\n",
        "fi=[]\n",
        "ku=[]\n",
        "tf=[]\n",
        "\n",
        "\n",
        "#Loops for Creation:\n",
        "for dic_names in Dictionaries_names: #Accessing each dictionary\n",
        "\n",
        "  #Rotation -> go to the dic and pull out each rotation\n",
        "  rot_list=Tachometer(eval(dic_names))\n",
        "  for x in rot_list: #Going inside of the list created by the Tachometer function\n",
        "    rot.append(x) #appending each term of the rot_list inside the rotation list for futher concatenation into the final dict\n",
        "\n",
        "  #Classification -> go to the dic and pull out each classification\n",
        "  classi=Clas(eval(dic_names))\n",
        "  for x in classi: #Going inside of the list created by the Clas function\n",
        "    clas.append(x) #appending each term of the classi_list inside the clas list for futher concatenation into the final dict\n",
        "\n",
        "  #RMS\n",
        "  RMS_list=RMS(eval(dic_names),data=all)\n",
        "  for x in RMS_list:\n",
        "    rms.append(x)\n",
        "\n",
        "  #Fator de Crista\n",
        "  FC_list=FCrista(eval(dic_names),data=all)\n",
        "  for x in FC_list:\n",
        "    fc.append(x)\n",
        "\n",
        "  #Fator de Impulso\n",
        "  FI_list=FImpulso(eval(dic_names),data=all)\n",
        "  for x in FI_list:\n",
        "    fi.append(x)\n",
        "\n",
        "  #Curtose\n",
        "  KU_list=Curtose(eval(dic_names),data=all)\n",
        "  for x in KU_list:\n",
        "    ku.append(x)\n",
        "\n",
        "  #Transformada\n",
        "  TF_list=fspectrum(eval(dic_names),data=all)\n",
        "  for x in TF_list:\n",
        "    tf.append(x)\n",
        "\n",
        "\n",
        "\n",
        "#Saindo dos Loops e Anexando tudo na Tabela Final:\n",
        "\n",
        "#Dados Tranquilos:\n",
        "df_ML['Classificação']=clas\n",
        "df_ML['Rotação']=rot\n",
        "\n",
        "#RMS:\n",
        "for names in n_c: #Nomes das Colunas\n",
        "  list_intermediaria=[] #Lista Intermediaria para colocar os valores dentro (Corresponde a coluna)\n",
        "  for i in range(len(rms)): #Acessando cada lista dentro da lista -> cada linha da tabela final\n",
        "    list_intermediaria.append(rms[i][n_c.index(names)]) #Varrendo os varios vetores e dando append na lista intermediaria -> criação da coluna\n",
        "  df_ML['RMS_'+names]=list_intermediaria #Inserindo dentro da tabela\n",
        "\n",
        "#Fator de Crista:\n",
        "for names in n_c: #Nomes das Colunas\n",
        "  list_intermediaria=[] #Lista Intermediaria para colocar os valores dentro (Corresponde a coluna)\n",
        "  for i in range(len(fc)): #Acessando cada lista dentro da lista -> cada linha da tabela final\n",
        "    list_intermediaria.append(fc[i][n_c.index(names)]) #Varrendo os varios vetores e dando append na lista intermediaria -> criação da coluna\n",
        "  df_ML['FC_'+names]=list_intermediaria #Inserindo dentro da tabela\n",
        "\n",
        "#Fator de Impulso:\n",
        "for names in n_c: #Nomes das Colunas\n",
        "  list_intermediaria=[] #Lista Intermediaria para colocar os valores dentro (Corresponde a coluna)\n",
        "  for i in range(len(fi)): #Acessando cada lista dentro da lista -> cada linha da tabela final\n",
        "    list_intermediaria.append(fi[i][n_c.index(names)]) #Varrendo os varios vetores e dando append na lista intermediaria -> criação da coluna\n",
        "  df_ML['FI_'+names]=list_intermediaria #Inserindo dentro da tabela\n",
        "\n",
        "#Curtose:\n",
        "for names in n_c: #Nomes das Colunas\n",
        "  list_intermediaria=[] #Lista Intermediaria para colocar os valores dentro (Corresponde a coluna)\n",
        "  for i in range(len(ku)): #Acessando cada lista dentro da lista -> cada linha da tabela final\n",
        "    list_intermediaria.append(ku[i][n_c.index(names)]) #Varrendo os varios vetores e dando append na lista intermediaria -> criação da coluna\n",
        "  df_ML['KU_'+names]=list_intermediaria #Inserindo dentro da tabela\n",
        "\n",
        "#FFT's:\n",
        "for names in n_c:\n",
        "  list_intermediaria=[]\n",
        "  for j in range(1250):\n",
        "    list_intermediaria=[]\n",
        "    for i in range(len(tf)):\n",
        "      list_intermediaria.append(tf[i][n_c.index(names)][j])\n",
        "    df_ML['FFT_'+str(j)+'_'+names]=list_intermediaria"
      ],
      "metadata": {
        "id": "fi0BhDFtjTZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ML"
      ],
      "metadata": {
        "id": "r-P-rl5sNIy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame(df_ML)"
      ],
      "metadata": {
        "id": "bo7ruyH9Niru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "xhI23bcrNmyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "Bh10E3lssh6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('DataframeML.csv')"
      ],
      "metadata": {
        "id": "hfhEeMPbny6r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}